{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b74cedc-a25b-45dd-beb3-5b4987a9611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willch/anaconda3/envs/pytorch_gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U einops datasets matplotlib tqdm\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c352acc8-1ca0-4a05-92a7-05fe927cdf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the helper functions\n",
    "\n",
    "def defaultValue(value, default_value):\n",
    "    if value is not None:\n",
    "        return value\n",
    "    return default_value() if isfunction(default_value) else default_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "52150f8c-55fb-4577-a0d1-2247e1177265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network helper components.\n",
    "\n",
    "# Residual net layer.\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, activation_fn):\n",
    "        super().__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.activation_fn(x, *args, **kwargs) + x\n",
    "\n",
    "# Conv upsampling layer, it upsamples the image by scale of 2 (twice).\n",
    "def Upsample(dim_in: int, dim_out=None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "        nn.Conv2d(dim_in, defaultValue(dim_out, dim_in), kernel_size=3, padding=1),\n",
    "    )\n",
    "\n",
    "# Conv downsampling layer, it downsamples the image by half.\n",
    "def Downsample(dim_in: int, dim_out=None):\n",
    "    # No More Strided Convolutions or Pooling\n",
    "    return nn.Sequential(\n",
    "        # Split each image into 4 smaller images, and stack all four sub-images into\n",
    "        # the channels.\n",
    "        Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2),\n",
    "        # Use the conv to sequeeze the dim as output one.\n",
    "        nn.Conv2d(dim * 4, defaultValue(dim_out, dim_in), 1),\n",
    "    )\n",
    "\n",
    "# Positional embedding layer.\n",
    "# We use the positional embedding to encode the timestamp t.\n",
    "# Here the timestamp means the t in the diffusion process.\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    # The time has dim: [B, 1], here 1 mean the correpsonding timestamp for that img.\n",
    "    # Recall the sin position embeddings are:\n",
    "    # PE(pos, 2i) = sin(pos/10000^{2i/d}) = sin(pos * exp (-1 * i * log(10000) / (d/2))).\n",
    "    # PE(pos, 2i+1) = cos(pos/10000^{2i/d}).\n",
    "    def forward(self, time: torch.Tensor):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "# Assemble the CNN layers\n",
    "class WeightStandardizedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/1903.10520\n",
    "    weight standardization purportedly works synergistically with group normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "\n",
    "        weight = self.weight\n",
    "        mean = reduce(weight, \"o ... -> o 1 1 1\", \"mean\")\n",
    "        var = reduce(weight, \"o ... -> o 1 1 1\", partial(torch.var, unbiased=False))\n",
    "        normalized_weight = (weight - mean) / (var + eps).rsqrt()\n",
    "        return F.conv2d(\n",
    "            x,\n",
    "            normalized_weight,\n",
    "            self.bias,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.groups,\n",
    "        )\n",
    "\n",
    "\n",
    "# This is a general CNN block which has group norm and SiLU.\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, scale_shift=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if scale_shift is not None:\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, dim_out: int, *, time_emb_dim=None, groups=8):\n",
    "        super().__init__()\n",
    "        # Applys a MLP to encode the time embeddings (i.e. SinusoidalPosition).\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))\n",
    "            if time_emb_dim is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, time_emb=None):\n",
    "        scale_shift = None\n",
    "        if self.mlp is not None and time_emb is not None:\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n",
    "            # Split the time embeddings into two tensors.\n",
    "            # One tensor is added as shift and another one is multiple as scale.\n",
    "            scale_shift = time_emb.chunk(2, dim=1)\n",
    "\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "# This layer uses CNN to encode/decode the input vectors.\n",
    "# Idea is to treat channels as the embedding/token vector, each pixel as one token.\n",
    "# conduct the self-attention among all pixels.\n",
    "class Attention(nn.Module):\n",
    "    # dim is the input tensor's channel dimensions.\n",
    "    # dim_head is the Q/K/V's dimension for each head.\n",
    "    def __init__(self, dim: int, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        # Note for conv, input is [B, C, H, W], and same for the output.\n",
    "        self.to_qkv = nn.Conv2d(in_channels=dim, out_channels=hidden_dim * 3, kernel_size=1, bias=False)\n",
    "        self.to_out = nn.Conv2d(in_channels=hidden_dim, out_channels=dim, kernel_size=1)\n",
    "\n",
    "    # We assume the input tensor has shape of [batch_size, channels, height, weight]\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b, c, h, w = x.shape\n",
    "        # Use the CNN layer to encode the QKV into a big tensor, then\n",
    "        # split it into 3 views (split by channel).\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        # Regroup the QKV tensors, here h means head.\n",
    "        # After mapping, QKV has shape [batch, heads, channels, x*y]\n",
    "        # thus, we only need to perform tranformer on the last dim.\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        # Q * K in the attention. Here means we perform attention along the channel\n",
    "        # dimention, for each pixel (i means the pixel in Q, j means pixel in K).\n",
    "        sim = einsum(\"b h c i, b h c j -> b h i j\", q, k)\n",
    "        # amax is argmax. Here we subtract the max dim to make softmax more numerical stable\n",
    "        # here we don't want the amax's gradient pass to the params, thus, uses detach.\n",
    "        # The detach just create a stopped gradient placerholder for that original tensor.\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h c j -> b h i c\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) c -> b (h c) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# See https://arxiv.org/abs/1812.01243.\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# Group norm layer. It's applied before transformer.\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8fa00f62-771f-45e0-ba94-9960364bc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the U-net. The input to the Unet is a noise image batches,\n",
    "# we expect it returns the noise. The input tensor shape is [B, channels, h, w].\n",
    "class Unet(nn.Module):\n",
    "    # dim: a general unit of the internal layer's dimenstion;\n",
    "    #      e.g. the channel num of downsample/upsample cnn.\n",
    "    # init_dim: there is a cnn as the first layer, it converts\n",
    "    #      input image channels to this init_dim.\n",
    "    # out_dim: the channels of final returned tensor. The U-net\n",
    "    #      is mirror for upsampling and downsampling. Thus, the image\n",
    "    #      size is the same as the noise size (w, h).\n",
    "    # channels: the number of the image channels;\n",
    "    # resnet_block_groups: the hyperparams for the group normalization\n",
    "    #      in the resnet sub-module.\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        init_dim=None,\n",
    "        out_dim=None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels=3,\n",
    "        self_condition=False,\n",
    "        resnet_block_groups=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = defaultValue(init_dim, dim)\n",
    "        self.init_conv = nn.Conv2d(input_channels, init_dim, 1, padding=0) # changed to 1 and 0 from 7,3\n",
    "\n",
    "        # The dims is [init_dim, dim, 2dim, 4dim, 8dim].\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        # The in_out is [(init_dim, dim), (dim, 2dim), (2dim, 4dim), (4dim, 8dim)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        print('in-out')\n",
    "        print(in_out)\n",
    "\n",
    "        resnet_block = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # time embeddings\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        # layers\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "\n",
    "        # Build stacks to downsample the images.\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        resnet_block(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        resnet_block(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        # Apply residual connection for norm layer + linear attention.\n",
    "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                        # Apply connv to downsample the image.\n",
    "                        # If not the last layer, we split image into 4 sub images and stack\n",
    "                        # them into channels to perform dim_out. Thus, image size shrink by half.\n",
    "                        Downsample(dim_in, dim_out)\n",
    "                        if not is_last\n",
    "                        # For the last layer, we simply conduct a conv2d, without change\n",
    "                        # the image size.\n",
    "                        else nn.Conv2d(dim_in, dim_out, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # The mid dim is 8 dim, the time dim is 4 dim.\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = resnet_block(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = resnet_block(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        resnet_block(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        resnet_block(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        # Similar as downsample, the non-last layer upsample double the image\n",
    "                        # size by nearest upsampling.\n",
    "                        Upsample(dim_out, dim_in)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.out_dim = defaultValue(out_dim, channels)\n",
    "\n",
    "        self.final_res_block = resnet_block(dim * 2, dim, time_emb_dim=time_dim)\n",
    "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
    "\n",
    "    # x is the input images with size as [B, channels, h, w];\n",
    "    # time is the timestamp tensor (in diffusion steps),  with size as [B, 1]\n",
    "    def forward(self, x: torch.Tensor, time: torch.Tensor, x_self_cond=None):\n",
    "        if self.self_condition:\n",
    "            x_self_cond = defaultValue(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            # Stack the zeros or identiy images into the channels.\n",
    "            x = torch.cat((x_self_cond, x), dim=1)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        # The clone will allow the gradients flow back.\n",
    "        # r will be directly stack to the last output layer as a residual connection.\n",
    "        residual_input_from_x = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        # we use h hold the output of downsample cnns.\n",
    "        # Those output will be passed to upsampler as a residual connection.\n",
    "        h = []\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            # Here we attach the output of downsample's cnn layer,\n",
    "            # it's similar as a residual connection.\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, residual_input_from_x), dim=1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9e3def85-24e6-4d51-b8b8-8c51def56d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward gaussain process (q).\n",
    "\n",
    "# Define the schedule of betas, which controls the strength of noise.\n",
    "# It return the 1D tensor whose size is total_timesteps.\n",
    "def linear_beta_schedule(total_timesteps: int):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, total_timesteps)\n",
    "\n",
    "class Alphas():\n",
    "    def __init__(self, total_timestamps: int):\n",
    "        super().__init__()\n",
    "        self.total_timestamps = total_timestamps\n",
    "        self.betas = linear_beta_schedule(total_timestamps)\n",
    "        alphas = 1. - self.betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "        self.sqrt_alpha_bars = torch.sqrt(alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_bars = torch.sqrt(1.0 - alphas_cumprod)\n",
    "\n",
    "    # We expect the t is a 2d tensor with shape [B, 1] or [B]\n",
    "    # We return [B, 1, ..., 1] tensor, the number of dimensions matches with image_shape.\n",
    "    # E.g. image_shape = [10, 3, 64, 64], then we return [10, 1, 1, 1].\n",
    "    def get_sqrt_alpha_bar_t(self, t: torch.Tensor, image_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        sqrt_alpha_bar_t = torch.gather(input=self.sqrt_alpha_bars, dim=-1, index=t)\n",
    "        return sqrt_alpha_bar_t.reshape(batch_size, *((1,) * (len(image_shape) - 1)))\n",
    "\n",
    "    def get_sqrt_one_minus_alphas_bar_t(self, t: torch.Tensor, image_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        sqrt_one_minus_alphas_bar_t = torch.gather(\n",
    "            input=self.sqrt_one_minus_alphas_bars, dim=-1, index=t)\n",
    "        return sqrt_one_minus_alphas_bar_t.reshape(\n",
    "            batch_size, *((1,) * (len(image_shape) - 1)))\n",
    "\n",
    "\n",
    "# forward diffusion: q(x_t | x_0)\n",
    "# q(x_t | x_0) = N(x_t; mean= sqrt(alpha_bar_t) * x_0, var =(1 - alpha_bar_t)I)\n",
    "def q_sample(x_start: torch.Tensor, t, alphas: Alphas, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alpha_bar_t = alphas.get_sqrt_alpha_bar_t( t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_bar_t = alphas.get_sqrt_one_minus_alphas_bar_t(\n",
    "        t, x_start.shape)\n",
    "\n",
    "    return sqrt_alpha_bar_t * x_start + sqrt_one_minus_alphas_bar_t * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c8c39-8fc0-4683-8459-d882cffcfa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image pre-process\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6baf6b5b-dfa2-4ce1-b871-c6502f0c03b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5452, 0.6225, 0.7769],\n",
      "        [0.4444, 0.4525, 0.3854]])\n",
      "(tensor([[0.5452],\n",
      "        [0.4444]]), tensor([[0.6225],\n",
      "        [0.4525]]), tensor([[0.7769],\n",
      "        [0.3854]]))\n",
      "torch.Size([2, 1])\n",
      "tensor([[[0.5452]],\n",
      "\n",
      "        [[0.4444]]])\n",
      "torch.Size([2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "b = a.chunk(3, dim = 1)\n",
    "print (b)\n",
    "print(b[0].size())\n",
    "\n",
    "q, k, v = map(lambda t: rearrange(t, \"(h c) x -> h c (x)\", h=2), b)\n",
    "print(q)\n",
    "print(q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8241b30b-4ab3-4afc-aabd-5cbfc3da49e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (2, 4), (3, 5)]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [3,4,5]\n",
    "c = zip(a,b)\n",
    "print(list(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "62a7ba1d-3a41-4725-beb9-b090a95fa3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-out\n",
      "[(64, 64), (64, 128), (128, 256)]\n"
     ]
    }
   ],
   "source": [
    "# quick test the Unet module.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dim = 64\n",
    "channels = 3\n",
    "batch_size = 10\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "model = Unet(\n",
    "    dim=dim,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 2, 4,)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0c4965d3-9e1a-4d84-a57c-9b8b309faaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 64, 64])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.rand(batch_size, channels, image_h, image_w)\n",
    "time = torch.ones(batch_size)\n",
    "print(test_input.shape)\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "edebf6d9-2194-4471-8c90-8982a428bbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "output = model.forward(test_input, time)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ecd02d61-ac5c-4de0-ab68-39de2be80e76",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "gather(): Expected dtype int64 for index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m x_start \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m      3\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size)\n\u001b[0;32m----> 4\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[43mq_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malphas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(q\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(q)\n",
      "Cell \u001b[0;32mIn[86], line 42\u001b[0m, in \u001b[0;36mq_sample\u001b[0;34m(x_start, t, alphas, noise)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m noise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x_start)\n\u001b[0;32m---> 42\u001b[0m sqrt_alpha_bar_t \u001b[38;5;241m=\u001b[39m \u001b[43malphas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sqrt_alpha_bar_t\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_start\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m sqrt_one_minus_alphas_bar_t \u001b[38;5;241m=\u001b[39m alphas\u001b[38;5;241m.\u001b[39mget_sqrt_one_minus_alphas_bar_t(\n\u001b[1;32m     44\u001b[0m     t, x_start\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sqrt_alpha_bar_t \u001b[38;5;241m*\u001b[39m x_start \u001b[38;5;241m+\u001b[39m sqrt_one_minus_alphas_bar_t \u001b[38;5;241m*\u001b[39m noise\n",
      "Cell \u001b[0;32mIn[86], line 25\u001b[0m, in \u001b[0;36mAlphas.get_sqrt_alpha_bar_t\u001b[0;34m(self, t, image_shape)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sqrt_alpha_bar_t\u001b[39m(\u001b[38;5;28mself\u001b[39m, t: torch\u001b[38;5;241m.\u001b[39mTensor, image_shape):\n\u001b[1;32m     24\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m     sqrt_alpha_bar_t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt_alpha_bars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sqrt_alpha_bar_t\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m*\u001b[39m((\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(image_shape) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: gather(): Expected dtype int64 for index"
     ]
    }
   ],
   "source": [
    "alphas = Alphas(300)\n",
    "x_start = output\n",
    "t = torch.ones(batch_size)\n",
    "q = q_sample(x_start, t, alphas)\n",
    "print(q.shape)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e5761b-dc1f-4c69-a50a-a5ad4f8d76f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
